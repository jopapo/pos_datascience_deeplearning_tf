{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.3 64-bit ('venv': virtualenv)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Deep Learning - Trabalho Final - Classificação de textos para análise de sentimentos",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "4082b515cb93b7843f5ff6f6ad97f95054605209e213f5ac575cc7757f0acb1a"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Classificação de textos para análise de sentimentos\n",
        "\n",
        "Trabalho final da disciplina de deep learning da pós graduação em data science da FURB. \n",
        "Professor: @luann.porfirio\n",
        "Aluno: João Poffo\n",
        "\n",
        "# Instruções do professor\n",
        "\n",
        "Base de dados \n",
        "\n",
        "Instruções:\n",
        "- O objetivo deste trabalho é criar um modelo binário de aprendizado de máquina para classificação de textos. \n",
        "Para isso, será utilizado a base de dados [IMDb](http://ai.stanford.edu/~amaas/data/sentiment/), que consiste de dados textuais de críticas positivas e negativas de filmes\n",
        "- Uma vez treinado, o modelo deve ter uma função `predict` que recebe uma string como parâmetro e retorna o valor 1 ou 0, aonde 1 significa uma crítica positiva e 0 uma crítica negativa\n",
        "- O pré-processamento pode ser desenvolvido conforme desejar (ex.: remoção de stopwords, word embedding, one-hot encoding, char encoding)\n",
        "- É preferível que seja empregado um modelo de recorrência (ex.: rnn, lstm, gru) para a etapa de classificação\n",
        "- Documente o código (explique sucintamente o que cada função faz, insira comentários em trechos de código relevantes)\n",
        "- **Atenção**: Uma vez treinado o modelo final, salve-o no diretório do seu projeto e crie uma célula ao final do notebook contendo uma função de leitura deste arquivo, juntamente com a execução da função `predict`\n",
        "\n",
        "Sugestões:\n",
        "- Explorar a base de dados nas células iniciais do notebook para ter um melhor entendimento do problema, distribuição dos dados, etc\n",
        "- Após desenvolver a estrutura de classificação, é indicado fazer uma busca de hiperparâmetros e comparar os resultados obtidos em diferentes situações\n",
        "\n",
        "Prazo de entrega:\n",
        "- 01-08-2021 às 23:59hs GMT-3\n",
        "\n",
        "Formato preferível de entrega:\n",
        "- Postar no portal Ava da disciplina o link do projeto no github (ou anexar o projeto diretamente no portal Ava)\n",
        "\n",
        "luann.porfirio@gmail.com"
      ],
      "metadata": {
        "id": "DK_cKZf9Dv4n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "source": [
        "# Instalando libs\r\n",
        "!pip install torchtext\r\n",
        "!pip install gensim\r\n",
        "!pip install pandas\r\n",
        "!pip install sklearn\r\n",
        "\r\n",
        "#Necessário se usássemos o vocabulário pré-treinado glove\r\n",
        "#!pip install spacy\r\n",
        "#!python -m spacy download en_core_web_sm"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://joao.poffo%40ambevtech.com.br:****@pkgs.dev.azure.com/AMBEV-SA/AMBEV-BIFROST/_packaging/canaa-packages/pypi/simple/\n",
            "Requirement already satisfied: torchtext in .\\venv\\lib\\site-packages (0.10.0)\n",
            "Requirement already satisfied: torch==1.9.0 in .\\venv\\lib\\site-packages (from torchtext) (1.9.0)\n",
            "Requirement already satisfied: numpy in .\\venv\\lib\\site-packages (from torchtext) (1.21.1)\n",
            "Requirement already satisfied: requests in .\\venv\\lib\\site-packages (from torchtext) (2.26.0)\n",
            "Requirement already satisfied: tqdm in .\\venv\\lib\\site-packages (from torchtext) (4.62.0)\n",
            "Requirement already satisfied: typing-extensions in .\\venv\\lib\\site-packages (from torch==1.9.0->torchtext) (3.10.0.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in .\\venv\\lib\\site-packages (from requests->torchtext) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in .\\venv\\lib\\site-packages (from requests->torchtext) (3.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in .\\venv\\lib\\site-packages (from requests->torchtext) (1.26.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in .\\venv\\lib\\site-packages (from requests->torchtext) (2021.5.30)\n",
            "Requirement already satisfied: colorama in .\\venv\\lib\\site-packages (from tqdm->torchtext) (0.4.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://joao.poffo%40ambevtech.com.br:****@pkgs.dev.azure.com/AMBEV-SA/AMBEV-BIFROST/_packaging/canaa-packages/pypi/simple/\n",
            "Requirement already satisfied: gensim in .\\venv\\lib\\site-packages (4.0.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in .\\venv\\lib\\site-packages (from gensim) (1.7.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in .\\venv\\lib\\site-packages (from gensim) (1.21.1)\n",
            "Requirement already satisfied: Cython==0.29.21 in .\\venv\\lib\\site-packages (from gensim) (0.29.21)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in .\\venv\\lib\\site-packages (from gensim) (5.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://joao.poffo%40ambevtech.com.br:****@pkgs.dev.azure.com/AMBEV-SA/AMBEV-BIFROST/_packaging/canaa-packages/pypi/simple/\n",
            "Requirement already satisfied: pandas in .\\venv\\lib\\site-packages (1.3.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in .\\venv\\lib\\site-packages (from pandas) (2021.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in .\\venv\\lib\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in .\\venv\\lib\\site-packages (from pandas) (1.21.1)\n",
            "Requirement already satisfied: six>=1.5 in .\\venv\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://joao.poffo%40ambevtech.com.br:****@pkgs.dev.azure.com/AMBEV-SA/AMBEV-BIFROST/_packaging/canaa-packages/pypi/simple/\n",
            "Requirement already satisfied: sklearn in .\\venv\\lib\\site-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in .\\venv\\lib\\site-packages (from sklearn) (0.24.2)\n",
            "Requirement already satisfied: joblib>=0.11 in .\\venv\\lib\\site-packages (from scikit-learn->sklearn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in .\\venv\\lib\\site-packages (from scikit-learn->sklearn) (1.7.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in .\\venv\\lib\\site-packages (from scikit-learn->sklearn) (1.21.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in .\\venv\\lib\\site-packages (from scikit-learn->sklearn) (2.2.0)\n"
          ]
        }
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-10T00:36:57.559764Z",
          "start_time": "2021-06-10T00:36:52.638020Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXoNI6TsDv4x",
        "outputId": "33f11002-55cb-4fb0-dd50-8198553a3c4b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# Bibliotecas necessárias\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "from torchtext.legacy import datasets\r\n",
        "from torchtext.legacy import data\r\n",
        "\r\n",
        "import random\r\n",
        "import pandas\r\n",
        "import gensim\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "import time\r\n"
      ],
      "outputs": [],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-10T00:44:12.514627Z",
          "start_time": "2021-06-10T00:44:12.509125Z"
        },
        "id": "NAsaMG8VDv41"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "source": [
        "# PREPARAÇÃO DOS DADOS\r\n",
        "# Primeiro teste é usando o gensim para tokenização. Usamos o gensim por causa do recurso de stemming.\r\n",
        "# Exemplo com pytorch (muito didático!): https://github.com/bentrevett/pytorch-sentiment-analysis\r\n",
        "# Exemplo do gensim: https://rohit-agrawal.medium.com/using-fine-tuned-gensim-word2vec-embeddings-with-torchtext-and-pytorch-17eea2883cd\r\n",
        "\r\n",
        "# Seed e Deterministic para que os testes possam ser reproduzidos. Em produção, é melhor e mais rápido desligar ambos.\r\n",
        "SEED = 789\r\n",
        "torch.manual_seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True\r\n",
        "\r\n",
        "# Definição dos filtros padrão do gensim. Estamos deixo abaixo para documentar a ordem.\r\n",
        "#gensim.parsing.preprocessing.DEFAULT_FILTERS = [\r\n",
        "#    lambda x: x.lower(), strip_tags, strip_punctuation,\r\n",
        "#    strip_multiple_whitespaces, strip_numeric,\r\n",
        "#    remove_stopwords, strip_short, stem_text\r\n",
        "#]\r\n",
        "\r\n",
        "# Função de tokenização\r\n",
        "def tokenize(sentence):\r\n",
        "    return gensim.parsing.preprocessing.preprocess_string(sentence)\r\n",
        "\r\n",
        "# Montando o tipo dos campos do torch\r\n",
        "\r\n",
        "# Aqui que associamos o tipo do campo com o gensim para construirmos o vocábulário adiante\r\n",
        "TEXT = data.Field(tokenize=tokenize)\r\n",
        "LABEL = data.LabelField(dtype = torch.float)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "pszVCeNEYaue"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "source": [
        "# Função para buscar os dados e quebrar em treino, teste e validação\r\n",
        "# - Necessário pois fazemos novos testes adiante.\r\n",
        "def train_test_valid():\r\n",
        "\r\n",
        "    # Trabalhando com o IMDb\r\n",
        "    train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\r\n",
        "\r\n",
        "    # Separa os dados de treino em treino e validação em 70/30\r\n",
        "    train_data, valid_data = train_data.split(random_state = random.seed(SEED))\r\n",
        "\r\n",
        "    return train_data, test_data, valid_data\r\n",
        "\r\n",
        "# Obtém os dados e faz o split\r\n",
        "train_data, test_data, valid_data = train_test_valid()\r\n"
      ],
      "outputs": [],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-10T00:50:27.424355Z",
          "start_time": "2021-06-10T00:49:16.448387Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSTFeBZ-Dv42",
        "outputId": "2aed927a-71f1-4210-8e95-0adbf60cbf96"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "source": [
        "# Verificação do split dos dados\r\n",
        "print('Exemplos de treino (35%):', len(train_data), ', teste (50%):', len(test_data), ' e validação (15%):', len(valid_data))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exemplos de treino (35%): 17500 teste (50%): 25000  e validação (15%): 7500\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCziwLteZxSP",
        "outputId": "13e3da21-99df-45c7-99ac-66e93390d3a4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "source": [
        "# Teste com vocabulário de 30 mil palavras\r\n",
        "MAX_VOCAB_SIZE = 30000\r\n",
        "\r\n",
        "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\r\n",
        "LABEL.build_vocab(train_data)\r\n",
        "\r\n",
        "print(\"Palavras únicas:\", len(TEXT.vocab))\r\n",
        "print(\"Classes únicas:\", len(LABEL.vocab))\r\n",
        "print(\"20 palavras mais comuns:\", TEXT.vocab.freqs.most_common(20))\r\n",
        "print(\"10 palavras do vocabulário de palavras para analisar estrutura:\", TEXT.vocab.itos[:10])\r\n",
        "print(\"Estrutura das classes:\", LABEL.vocab.stoi)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palavras únicas: 30002\n",
            "Classes únicas: 2\n",
            "20 palavras mais comuns: [('movi', 36384), ('film', 34043), ('like', 16064), ('time', 11242), ('good', 10752), ('charact', 9860), ('watch', 9781), ('stori', 9233), ('scene', 7433), ('look', 7114), ('end', 6838), ('bad', 6587), ('peopl', 6489), ('great', 6383), ('love', 6285), ('think', 6262), ('wai', 6192), ('act', 6183), ('plai', 6165), ('thing', 5778)]\n",
            "10 palavras do vocabulário de palavras para analisar estrutura: ['<unk>', '<pad>', 'movi', 'film', 'like', 'time', 'good', 'charact', 'watch', 'stori']\n",
            "Estrutura das classes: defaultdict(None, {'neg': 0, 'pos': 1})\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klynVEagdHQA",
        "outputId": "53473727-9c95-4d45-f9a9-c741d29cf254"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "source": [
        "# Criando lotes de 64 através do iterador para o treino\r\n",
        "BATCH_SIZE = 64\r\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\r\n",
        "    (train_data, valid_data, test_data), \r\n",
        "    batch_size = BATCH_SIZE)\r\n",
        "\r\n",
        "# Classe de rede neural recorrente padrão simples\r\n",
        "class RNN(nn.Module):\r\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\r\n",
        "        \r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        # Usamos o embedding pois é uma camada pra transformar um vetor esparço de dicionário em um denso. \r\n",
        "        # Na teoria, palavras com impacto similar na classificação dos sentimentos são mapeadas próximas uma das outras.\r\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\r\n",
        "        \r\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\r\n",
        "        \r\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\r\n",
        "        \r\n",
        "    def forward(self, text):\r\n",
        "\r\n",
        "        #text = [sent len, batch size]\r\n",
        "        embedded = self.embedding(text)\r\n",
        "        \r\n",
        "        #embedded = [sent len, batch size, emb dim]\r\n",
        "        output, hidden = self.rnn(embedded)\r\n",
        "        \r\n",
        "        #output = [sent len, batch size, hid dim]\r\n",
        "        #hidden = [1, batch size, hid dim]        \r\n",
        "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\r\n",
        "        \r\n",
        "        return self.fc(hidden.squeeze(0))\r\n",
        "\r\n",
        "# A quantidade features é a quantidade de palavras no nosso dicionário\r\n",
        "INPUT_DIM = len(TEXT.vocab)\r\n",
        "\r\n",
        "# Quão denso deve ser nosso embeeding\r\n",
        "EMBEDDING_DIM = 100\r\n",
        "\r\n",
        "# Tamanho da cama oculta\r\n",
        "HIDDEN_DIM = 256\r\n",
        "\r\n",
        "# Tamanho da saída\r\n",
        "OUTPUT_DIM = 1\r\n",
        "\r\n",
        "# Cria o modelo com os hiperparâmetros definidos acima\r\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\r\n",
        "\r\n",
        "# Imprime a arquitetura\r\n",
        "print(model)\r\n",
        "\r\n",
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "print(f'Esta modelo tem {count_parameters(model):,} parâmetros treináveis.')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN(\n",
            "  (embedding): Embedding(30002, 100)\n",
            "  (rnn): RNN(100, 256)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            ")\n",
            "Esta modelo tem 3,092,105 parâmetros treináveis.\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQBjIsDcff2T",
        "outputId": "350932a4-682b-440a-d7e5-41e68f7c160d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "source": [
        "# No primeiro teste vamos usar um otimizador simples - gradiente descendente estocástico\r\n",
        "# - learning_rate = 0.001\r\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)\r\n",
        "\r\n",
        "# Como loss usamos este do torch que traz tanto a sigmoide quanto a entropia cruzada binária.\r\n",
        "criterion = nn.BCEWithLogitsLoss()\r\n",
        "\r\n",
        "# Função para calcular acurácia binária. \r\n",
        "def binary_accuracy(preds, y):\r\n",
        "    \"\"\"\r\n",
        "    Retorna a acurácia no lote. Por exemplo, 6/10 corretas, retorna 0.6.\r\n",
        "    Ao mesmo tempo traduz a predição para uma das classes binárias disponíveis através de arredondamento. Por exemplo: 0.6 -> 1 (verdadeiro), 0.3 -> 0 (falso).\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\r\n",
        "    correct = (rounded_preds == y).float()\r\n",
        "    acc = correct.sum() / len(correct)\r\n",
        "    return acc\r\n",
        "\r\n",
        "# Função de treino recebendo o modelo, os dados, o otimizador e a loss como argumentos\r\n",
        "def train(model, iterator, optimizer, criterion):\r\n",
        "    \r\n",
        "    # loss e acurácia acumulada\r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_acc = 0\r\n",
        "    \r\n",
        "    # Seta o modelo para o modo de treino\r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    # Para cada lote de dados\r\n",
        "    for batch in iterator:\r\n",
        "        \r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        predictions = model(batch.text).squeeze(1)\r\n",
        "        \r\n",
        "        loss = criterion(predictions, batch.label)\r\n",
        "        \r\n",
        "        acc = binary_accuracy(predictions, batch.label)\r\n",
        "        \r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        epoch_loss += loss.item()\r\n",
        "        epoch_acc += acc.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\r\n",
        "\r\n",
        "# Função de validação\r\n",
        "def evaluate(model, iterator, criterion):\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_acc = 0\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for batch in iterator:\r\n",
        "\r\n",
        "            predictions = model(batch.text).squeeze(1)\r\n",
        "            \r\n",
        "            loss = criterion(predictions, batch.label)\r\n",
        "            \r\n",
        "            acc = binary_accuracy(predictions, batch.label)\r\n",
        "\r\n",
        "            epoch_loss += loss.item()\r\n",
        "            epoch_acc += acc.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\r\n",
        "\r\n",
        "# Função para ajudar no cálculo de cada época\r\n",
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs\r\n",
        "\r\n",
        "# Quantidade de épocas usadas no treino\r\n",
        "N_EPOCHS = 5\r\n",
        "\r\n",
        "# Treino está como um processo pois é padrão\r\n",
        "def process(model, train_fnc=train, evaluate_fnc=evaluate, save_name='tf-model1.pt'):\r\n",
        "    # Ajuda a identificar a melhor época para salvar o modelo\r\n",
        "    best_valid_loss = float('inf')\r\n",
        "\r\n",
        "    for epoch in range(N_EPOCHS):\r\n",
        "\r\n",
        "        start_time = time.time()\r\n",
        "        \r\n",
        "        train_loss, train_acc = train_fnc(model, train_iterator, optimizer, criterion)\r\n",
        "        valid_loss, valid_acc = evaluate_fnc(model, valid_iterator, criterion)\r\n",
        "        \r\n",
        "        end_time = time.time()\r\n",
        "\r\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "        \r\n",
        "        if valid_loss < best_valid_loss:\r\n",
        "            best_valid_loss = valid_loss\r\n",
        "            torch.save(model.state_dict(), save_name)\r\n",
        "        \r\n",
        "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\r\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\r\n",
        "\r\n",
        "# Executa o treino do modelo\r\n",
        "process(model)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not tuple",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4708/1881583158.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;31m# Executa o treino do modelo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m \u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4708/1881583158.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(model, train_fnc, evaluate_fnc, save_name)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fnc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_fnc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4708/1881583158.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32me:\\temp\\td_deeplearning\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4708/33533069.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m#text = [sent len, batch size]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0membedded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m#embedded = [sent len, batch size, emb dim]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32me:\\temp\\td_deeplearning\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32me:\\temp\\td_deeplearning\\venv\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    158\u001b[0m         return F.embedding(\n\u001b[0;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32me:\\temp\\td_deeplearning\\venv\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2041\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2042\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2043\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not tuple"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2IBFNdagLdq",
        "outputId": "5a91fe49-cff9-4b0e-b14f-ee0a6b624f42"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "source": [
        "# Muito ruim com as N palavras mais frequentes\r\n",
        "\r\n",
        "import torch.nn as nn\r\n",
        "\r\n",
        "class MultiLayerRNN(nn.Module):\r\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \r\n",
        "                 bidirectional, dropout, pad_idx):\r\n",
        "        \r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\r\n",
        "        \r\n",
        "        self.rnn = nn.LSTM(embedding_dim, \r\n",
        "                           hidden_dim, \r\n",
        "                           num_layers=n_layers, \r\n",
        "                           bidirectional=bidirectional, \r\n",
        "                           dropout=dropout)\r\n",
        "        \r\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, text, text_lengths):\r\n",
        "        \r\n",
        "        #text = [sent len, batch size]\r\n",
        "        \r\n",
        "        embedded = self.dropout(self.embedding(text))\r\n",
        "        \r\n",
        "        #embedded = [sent len, batch size, emb dim]\r\n",
        "        \r\n",
        "        #pack sequence\r\n",
        "        # lengths need to be on CPU!\r\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\r\n",
        "        \r\n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\r\n",
        "        \r\n",
        "        #unpack sequence\r\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\r\n",
        "\r\n",
        "        #output = [sent len, batch size, hid dim * num directions]\r\n",
        "        #output over padding tokens are zero tensors\r\n",
        "        \r\n",
        "        #hidden = [num layers * num directions, batch size, hid dim]\r\n",
        "        #cell = [num layers * num directions, batch size, hid dim]\r\n",
        "        \r\n",
        "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\r\n",
        "        #and apply dropout\r\n",
        "        \r\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\r\n",
        "                \r\n",
        "        #hidden = [batch size, hid dim * num directions]\r\n",
        "            \r\n",
        "        return self.fc(hidden)\r\n",
        "\r\n",
        "def train_with_sequences(model, iterator, optimizer, criterion):\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_acc = 0\r\n",
        "    \r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    for batch in iterator:\r\n",
        "        \r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        text, text_lengths = batch.text\r\n",
        "        \r\n",
        "        predictions = model(text, text_lengths).squeeze(1)\r\n",
        "        \r\n",
        "        loss = criterion(predictions, batch.label)\r\n",
        "        \r\n",
        "        acc = binary_accuracy(predictions, batch.label)\r\n",
        "        \r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        epoch_loss += loss.item()\r\n",
        "        epoch_acc += acc.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\r\n",
        "\r\n",
        "def evaluate_with_sequences(model, iterator, criterion):\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_acc = 0\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for batch in iterator:\r\n",
        "\r\n",
        "            text, text_lengths = batch.text\r\n",
        "            \r\n",
        "            predictions = model(text, text_lengths).squeeze(1)\r\n",
        "            \r\n",
        "            loss = criterion(predictions, batch.label)\r\n",
        "            \r\n",
        "            acc = binary_accuracy(predictions, batch.label)\r\n",
        "\r\n",
        "            epoch_loss += loss.item()\r\n",
        "            epoch_acc += acc.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\r\n",
        "\r\n",
        "# Testando melhorando nossa rede com LSTM. Para isso precisamos incluir a quantidade de sentenças no vocabulário (include_length=true).\r\n",
        "TEXT = data.Field(tokenize = tokenize,\r\n",
        "                  include_lengths = True)\r\n",
        "\r\n",
        "# Precisa recarregar dados\r\n",
        "train_data, test_data, valid_data = train_test_valid()\r\n",
        "\r\n",
        "TEXT.build_vocab(train_data, \r\n",
        "    max_size = MAX_VOCAB_SIZE, \r\n",
        "    # Teste com o glove fica pra depois\r\n",
        "    #vectors = \"glove.6B.100d\", \r\n",
        "    #unk_init = torch.Tensor.normal_\r\n",
        "    )\r\n",
        "LABEL.build_vocab(train_data)\r\n",
        "\r\n",
        "#INPUT_DIM = len(TEXT.vocab)\r\n",
        "#EMBEDDING_DIM = 100\r\n",
        "#HIDDEN_DIM = 256\r\n",
        "#OUTPUT_DIM = 1\r\n",
        "N_LAYERS = 2\r\n",
        "BIDIRECTIONAL = True\r\n",
        "DROPOUT = 0.5\r\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\r\n",
        "\r\n",
        "model2 = MultiLayerRNN(INPUT_DIM, \r\n",
        "            EMBEDDING_DIM, \r\n",
        "            HIDDEN_DIM, \r\n",
        "            OUTPUT_DIM, \r\n",
        "            N_LAYERS, \r\n",
        "            BIDIRECTIONAL, \r\n",
        "            DROPOUT, \r\n",
        "            PAD_IDX)\r\n",
        "\r\n",
        "#pretrained_embeddings = TEXT.vocab.vectors\r\n",
        "\r\n",
        "#print(pretrained_embeddings.shape)\r\n",
        "\r\n",
        "#model2.embedding.weight.data.copy_(pretrained_embeddings)\r\n",
        "\r\n",
        "#UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\r\n",
        "\r\n",
        "#model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\r\n",
        "#model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\r\n",
        "\r\n",
        "#print(model.embedding.weight.data)\r\n",
        "\r\n",
        "optimizer = optim.Adam(model2.parameters())\r\n",
        "\r\n",
        "# Another thing for packed padded sequences all of the tensors within a batch need to be sorted by their lengths. This is handled in the iterator by setting sort_within_batch = True.\r\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\r\n",
        "    (train_data, valid_data, test_data), \r\n",
        "    batch_size = BATCH_SIZE,\r\n",
        "    sort_within_batch = True)\r\n",
        "\r\n",
        "process(model2, train_with_sequences, evaluate_with_sequences, 'tut2-model.pt')\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 65m 56s\n",
            "\tTrain Loss: 0.641 | Train Acc: 63.00%\n",
            "\t Val. Loss: 0.572 |  Val. Acc: 72.01%\n",
            "Epoch: 02 | Epoch Time: 58m 7s\n",
            "\tTrain Loss: 0.537 | Train Acc: 72.92%\n",
            "\t Val. Loss: 0.477 |  Val. Acc: 78.10%\n",
            "Epoch: 03 | Epoch Time: 57m 58s\n",
            "\tTrain Loss: 0.450 | Train Acc: 79.18%\n",
            "\t Val. Loss: 0.412 |  Val. Acc: 82.54%\n",
            "Epoch: 04 | Epoch Time: 57m 7s\n",
            "\tTrain Loss: 0.414 | Train Acc: 81.33%\n",
            "\t Val. Loss: 0.387 |  Val. Acc: 83.98%\n",
            "Epoch: 05 | Epoch Time: 57m 41s\n",
            "\tTrain Loss: 0.371 | Train Acc: 84.23%\n",
            "\t Val. Loss: 0.403 |  Val. Acc: 83.85%\n"
          ]
        }
      ],
      "metadata": {
        "id": "a8McPbOQ8HFn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Usando o word2vec do gensim para treinar um modelo\r\n",
        "# Exemplo principal em: https://www.kaggle.com/guichristmann/lstm-classification-model-with-word2vec\r\n",
        "\r\n",
        "embedding_dim = 100\r\n",
        "window_size = 10\r\n",
        "min_count = 1\r\n",
        "\r\n",
        "w2v_model = gensim.models.Word2Vec(sentences=list(train_data.text), size=embedding_dim, window=window_size, min_count=min_count, workers=4)\r\n",
        "w2v_weights = w2v_model.wv.vectors\r\n",
        "vocab_size, embedding_size = w2v_weights.shape\r\n",
        "\r\n",
        "print(\"Vocabulary Size: {} - Embedding Dim: {}\".format(vocab_size, embedding_size))\r\n",
        "\r\n",
        "print('good', w2v_model.wv.most_similar('good')) \r\n",
        "print('bad', w2v_model.wv.most_similar('bad'))\r\n",
        "# Não entendi corretamente esse bad similar a good (deve ser quando for not bad) - mas no geral a similaridade ficou muito boa. Mas vamos ver como treinar.\r\n",
        "\r\n",
        "import torchtext.vocab as vocab\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "KdT2dZdykxqr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "  bad_words = w2v_model.wv.most_similar('bad', topn=100)\r\n",
        "\r\n",
        "w2v_model.wv.sy\r\n",
        "\r\n",
        "#from torchtext.vocab import Vectors\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "vectors = []\r\n",
        "dim = np.array([])\r\n",
        "stoi = {}\r\n",
        "for seq in bad_words:\r\n",
        "  vectors.append(seq[0])\r\n",
        "  dim = np.append(dim, seq[1])\r\n",
        "  stoi[seq[0]] = len(stoi)\r\n",
        "\r\n",
        "\r\n",
        "#TEXT.build_vocab(train_data, max_size = len(bad_words_vec), vectors=bad_words_vec)\r\n",
        "TEXT.vocab.set_vectors(stoi, vectors, (100))"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZrnKSQKj1R_U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model.load_state_dict(torch.load('tut1-model.pt'))\r\n",
        "\r\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\r\n",
        "\r\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\r\n",
        "\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "Hc3cKyZrhRcg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Transforma em dataframe para brincarmos com os dados\r\n",
        "columns = ['sentiment', 'text']\r\n",
        "train_df = pandas.DataFrame(train_iter, columns=columns)\r\n",
        "test_df = pandas.DataFrame(test_iter, columns=columns)\r\n",
        "\r\n",
        "# Dando uma olhada nos registros\r\n",
        "train_df"
      ],
      "outputs": [],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-10T00:51:00.041250Z",
          "start_time": "2021-06-10T00:51:00.016619Z"
        },
        "id": "ycuK46z6Dv43"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Só pra verificar quais opções temos: pos (assumo positiva) e neg (negativa).\r\n",
        "train_df.sentiment.value_counts()"
      ],
      "outputs": [],
      "metadata": {
        "id": "QxWk3tdIGayx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#train_model = gensim.models.Word2Vec(train_df[1])\r\n",
        "\r\n",
        "# Os filtros padrão são o suficiente no momento\r\n",
        "#gensim.parsing.preprocessing.DEFAULT_FILTERS = [\r\n",
        "#    lambda x: x.lower(), strip_tags, strip_punctuation,\r\n",
        "#    strip_multiple_whitespaces, strip_numeric,\r\n",
        "#    remove_stopwords, strip_short, stem_text\r\n",
        "#]\r\n",
        "\r\n",
        "def preprocessor(text):\r\n",
        "  return gensim.parsing.preprocessing.preprocess_string(text)\r\n",
        "\r\n",
        "# Como fala no filtro acima, primeiro coloca em minúsculo, depois remove tags, pontuação, espaço duplo, números, palavras irrelevantes e faz o steeming\r\n",
        "train_df['preprocessed_words'] = train_df.text.apply(lambda text : preprocessor(text))\r\n",
        "test_df['preprocessed_words'] = test_df.text.apply(lambda text : preprocessor(text))\r\n",
        "\r\n",
        "train_df"
      ],
      "outputs": [],
      "metadata": {
        "id": "3CqvHtMKIRXw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Primeiro teste. TfIdf por cenário: pos/neg\r\n",
        "\r\n",
        "pos_vectorizer = TfidfVectorizer() #tokenizer=gensim.parsing.preprocessing.preprocess_string)\r\n",
        "X = pos_vectorizer.fit_transform(train_df[train_df.sentiment == 'pos'].text)\r\n",
        "print(pos_vectorizer.get_feature_names())\r\n",
        "print(X.shape)\r\n",
        "\r\n",
        "neg_vectorizer = TfidfVectorizer() #tokenizer=gensim.parsing.preprocessing.preprocess_string)\r\n",
        "X = neg_vectorizer.fit_transform(train_df[train_df.sentiment == 'neg'].text)\r\n",
        "\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "g73Y6hYqNm6-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Testando a pontuação das palavras em cada cenário\r\n",
        "testing_words = ['good', 'bad', 'nice', 'obnoxious', 'preview', 'old', 'luiz', 'not', 'no']\r\n",
        "\r\n",
        "print('word', 'positive', 'negative')\r\n",
        "for w in testing_words : print(w, pos_vectorizer.vocabulary_.get(w,0), neg_vectorizer.vocabulary_.get(w, 0))\r\n",
        "\r\n",
        "# A pontuação referente ao Tfidf não é boa por esse objetivo pois a ideia geral são as palavras que estão presentes frequentemente porém, em menos documentos. Normalmente são palavras definidoras de um documento.\r\n",
        "# Assim a pontuação abaixo mostra que o Tfidf é sempre um pouco mais alto no pos. Independente do conceito associado."
      ],
      "outputs": [],
      "metadata": {
        "id": "ai8CpbYET9lf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Vamos varrer o dataset de treinamento pra tentar analizar a frequência por palavra\r\n",
        "group = train_df.explode('preprocessed_words').groupby([\"preprocessed_words\", \"sentiment\"]).count()\r\n",
        "group = group.unstack().fillna(0)\r\n",
        "group.sort_values([('text', 'neg')], ascending=False) \r\n",
        "\r\n",
        "# Curioso aqui é que palavras como like e good aparecem com frequências similares em ambos os sentimentos\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "TfEA9CiLUA_e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Usando o word2vec do gensim para treinar um modelo\r\n",
        "# Exemplo principal em: https://www.kaggle.com/guichristmann/lstm-classification-model-with-word2vec\r\n",
        "\r\n",
        "embedding_dim = 100\r\n",
        "window_size = 10\r\n",
        "min_count = 1\r\n",
        "\r\n",
        "w2v_model = gensim.models.Word2Vec(sentences=train_df['preprocessed_words'], size=embedding_dim, window=window_size, min_count=min_count, workers=4)\r\n",
        "w2v_weights = w2v_model.wv.vectors\r\n",
        "vocab_size, embedding_size = w2v_weights.shape\r\n",
        "\r\n",
        "print(\"Vocabulary Size: {} - Embedding Dim: {}\".format(vocab_size, embedding_size))"
      ],
      "outputs": [],
      "metadata": {
        "id": "fFJAe9PCrj3h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print('good', w2v_model.wv.most_similar('good')) \r\n",
        "print('bad', w2v_model.wv.most_similar('bad'))\r\n",
        "\r\n",
        "# Não entendi corretamente esse bad similar a good (deve ser quando for not bad) - mas no geral a similaridade ficou muito boa. Mas vamos ver como treinar."
      ],
      "outputs": [],
      "metadata": {
        "id": "P2e4Cp0jtNlg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np \r\n",
        "\r\n",
        "def encode_word(word):\r\n",
        "  return w2v_model.wv.distance(word, 'bad')\r\n",
        "\r\n",
        "def encode_words(words):\r\n",
        "  return np.array([encode_word(w) for w in words])\r\n",
        "\r\n",
        "def vocab_size():\r\n",
        "  return len(w2v_model.wv.vocab)\r\n",
        "\r\n",
        "# Funções de apoio pra transformar de palavra para código e de código para palavra\r\n",
        "def word2token(word):\r\n",
        "    try:\r\n",
        "        return w2v_model.wv.vocab[word].index\r\n",
        "    except KeyError:\r\n",
        "        return -1\r\n",
        "\r\n",
        "def token2word(token):\r\n",
        "    return w2v_model.wv.index2word[token]  \r\n",
        "\r\n",
        "def to_x(preprocessed_words):\r\n",
        "  data = np.zeros(vocab_size())\r\n",
        "  for w in preprocessed_words:\r\n",
        "    index = word2token(w)\r\n",
        "    if index < 0:\r\n",
        "      ow = w2v_model.wv.most_similar(w, topn=1)[0]\r\n",
        "      index = word2token(ow)\r\n",
        "    if index >= 0:\r\n",
        "      data[index] = 1\r\n",
        "  return torch.from_numpy(data)\r\n",
        "\r\n",
        "def to_y(sentiment):\r\n",
        "  return torch.from_numpy(np.array([1 if sentiment == 'pos' else 0]))\r\n",
        "\r\n",
        "words = train_df.preprocessed_words[0]\r\n",
        "encoded = encode_words(words)\r\n",
        "\r\n",
        "# Testando o encoding\r\n",
        "print(words[:20])\r\n",
        "print(encoded[:20])\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "7_3hIArtO8a4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from torch import nn\r\n",
        "\r\n",
        "class WordRNN(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, n_hidden=256, n_layers=2,\r\n",
        "                               drop_prob=0.5, lr=0.001):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.drop_prob = drop_prob\r\n",
        "        self.n_layers = n_layers\r\n",
        "        self.n_hidden = n_hidden\r\n",
        "        self.lr = lr\r\n",
        "        \r\n",
        "        input_size = vocab_size()\r\n",
        "        \r\n",
        "        #definir lstm -> input_size, hidden_size, num_layers batch_first\r\n",
        "        self.lstm = nn.LSTM(input_size, n_hidden, n_layers, batch_first=True)\r\n",
        "        \r\n",
        "        #definir dropout -> drop_prob\r\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\r\n",
        "        \r\n",
        "        #definir camada fc -> num_hidden input_size\r\n",
        "        self.fc = nn.Linear(n_hidden, input_size, '')\r\n",
        "      \r\n",
        "    \r\n",
        "    def forward(self, x, hidden):\r\n",
        "               \r\n",
        "        # camada lstm\r\n",
        "        r_output, hidden = self.lstm(x, hidden)\r\n",
        "        \r\n",
        "        # dropout\r\n",
        "        out = self.dropout(r_output)\r\n",
        "        \r\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\r\n",
        "        \r\n",
        "        # camada fc\r\n",
        "        out = self.fc(out)\r\n",
        "        \r\n",
        "        return out, hidden\r\n",
        "    \r\n",
        "    \r\n",
        "    def init_hidden(self, batch_size):\r\n",
        "        # Gera tensores de tamanho n_layers x betch_size x n_hidden\r\n",
        "        weight = next(self.parameters()).data\r\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\r\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\r\n",
        "        return hidden\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "tNHWDkGUwPrD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class RNN(nn.Module):\r\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\r\n",
        "        '''\r\n",
        "        input_size - dimensao da entrada\r\n",
        "        hidden_dim - numero de features\r\n",
        "        n_layers - numero de camadas RNN (usualmente entre 1 e 3)\r\n",
        "        '''\r\n",
        "        super(RNN, self).__init__()\r\n",
        "        self.hidden_dim = hidden_dim\r\n",
        "        #RNN\r\n",
        "        #batch_first - batch_size na primeira dimensão da entrada: (batch_size, seq_length, hidden_dim)\r\n",
        "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\r\n",
        "        #fully-connected\r\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\r\n",
        "\r\n",
        "        \r\n",
        "    def forward(self, x, hidden):\r\n",
        "        '''\r\n",
        "        x - (batch_size, seq_length, input_size)\r\n",
        "        hidden - (n_layers, batch_size, hidden_dim)\r\n",
        "        r_out - (batch_size, time_step, hidden_size)\r\n",
        "        '''\r\n",
        "        \r\n",
        "        batch_size = x.size(0)\r\n",
        "        \r\n",
        "        #Saida RNN\r\n",
        "        r_out, hidden = self.rnn(x, hidden)\r\n",
        "        \r\n",
        "        #reshape: (batch_size*seq_length, hidden_dim)\r\n",
        "        r_out = r_out.view(-1, self.hidden_dim)  \r\n",
        "        \r\n",
        "        #prediction\r\n",
        "        output = self.fc(r_out)\r\n",
        "        \r\n",
        "        return output, hidden"
      ],
      "outputs": [],
      "metadata": {
        "id": "5Gtr2zfH5LEs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def train(net, data, epochs=10, batch_size=10, lr=0.001, clip=5, val_frac=0.1, print_every=10):\r\n",
        "    net.train()\r\n",
        "    \r\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\r\n",
        "    criterion = nn.CrossEntropyLoss()\r\n",
        "    \r\n",
        "    n_words = vocab_size()\r\n",
        "    for e in range(epochs):\r\n",
        "        h = net.init_hidden(batch_size)\r\n",
        "        \r\n",
        "        for counter, row in data.iterrows():\r\n",
        "            \r\n",
        "            # One-hot encoding\r\n",
        "            x = to_x(row.preprocessed_words)\r\n",
        "            x = x.reshape(1, 1, len(x))\r\n",
        "            #print(x)\r\n",
        "            y = to_y(row.sentiment)\r\n",
        "            #print(y)\r\n",
        "            \r\n",
        "            # Cria variáveis para hidden state \r\n",
        "            #h = tuple([each.data for each in h])\r\n",
        "            #print(h)\r\n",
        "\r\n",
        "            net.zero_grad()            \r\n",
        "            \r\n",
        "            # saida do modelo\r\n",
        "            output, h = net(x, h)\r\n",
        "            \r\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\r\n",
        "            loss.backward()\r\n",
        "            \r\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\r\n",
        "            opt.step()\r\n",
        "            \r\n",
        "            #validacao\r\n",
        "            if counter % print_every == 0:\r\n",
        "                val_h = net.init_hidden(batch_size)\r\n",
        "                val_losses = []\r\n",
        "                net.eval()\r\n",
        "\r\n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\r\n",
        "                      \"Step: {}...\".format(counter),\r\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\r\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\r\n",
        "                \r\n",
        "net = WordRNN()\r\n",
        "print(net)\r\n",
        "\r\n",
        "n_epochs = 10\r\n",
        "\r\n",
        "train(net, train_df, epochs=n_epochs, batch_size=input_size, lr=0.001, print_every=10)"
      ],
      "outputs": [],
      "metadata": {
        "id": "u25Vj6GewreR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def train(rnn, data, n_steps, print_every):\r\n",
        "    \r\n",
        "    # initializa o hidden state\r\n",
        "    hidden = None      \r\n",
        "\r\n",
        "    for batch_i, step in enumerate(range(n_steps)):\r\n",
        "\r\n",
        "      for counter, row in data.iterrows():\r\n",
        "\r\n",
        "        x = to_x(row.preprocessed_words)\r\n",
        "        y = to_y(row.sentiment)\r\n",
        "\r\n",
        "        #saida do bloco RNN\r\n",
        "        prediction, hidden = rnn(x, hidden)\r\n",
        "        hidden = hidden.data\r\n",
        "\r\n",
        "        # calcula loss\r\n",
        "        loss = criterion(prediction, y)\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        #backpropagation e atualização dos pesos\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        #display\r\n",
        "        if batch_i%print_every == 0:        \r\n",
        "            print('Loss: ', loss.item())\r\n",
        "            plt.plot(time_steps[1:], x, 'r.')\r\n",
        "            plt.plot(time_steps[1:], prediction.data.numpy().flatten(), 'b.')\r\n",
        "            plt.show()\r\n",
        "\r\n",
        "    return rnn\r\n",
        "\r\n",
        "#hiperparametros\r\n",
        "n_steps = 10\r\n",
        "input_size = len(w2v_model.wv.vocab)\r\n",
        "output_size = 1\r\n",
        "hidden_dim = 255\r\n",
        "n_layers = 2\r\n",
        "print_every = 30\r\n",
        "\r\n",
        "net = RNN(input_size=len(w2v_model.wv.vocab) , output_size=1, hidden_dim=hidden_dim, n_layers = n_layers)\r\n",
        "print(net)\r\n",
        "\r\n",
        "trained_rnn = train(net, train_df, n_steps, print_every)"
      ],
      "outputs": [],
      "metadata": {
        "id": "6rJrjtu15tlY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class RNN2(nn.Module):\r\n",
        "    def __init__(self, input_size, hidden_size, vocab_size, output_size):\r\n",
        "        super(RNN2, self).__init__()\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, input_size)\r\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\r\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\r\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\r\n",
        "    def forward(self, word, hidden):\r\n",
        "        embeds = self.word_embeddings(word)\r\n",
        "        combined = torch.cat((embeds.view(1, -1), hidden), 1)\r\n",
        "        hidden = self.i2h(combined)\r\n",
        "        output = self.i2o(combined)\r\n",
        "        output = self.softmax(output)\r\n",
        "        return output, hidden\r\n",
        "    def init_hidden(self):\r\n",
        "        return torch.zeros(1, self.hidden_size)\r\n",
        "\r\n",
        "\r\n",
        "# Pense no embeeding como uma lookup table (https://www.tensorflow.org/text/guide/word_embeddings)\r\n",
        "EMBEDDING_DIM = len(w2v_model.wv.vectors[0])\r\n",
        "HIDDEN_DIM = 10\r\n",
        "output_size = 2\r\n",
        "\r\n",
        "# creating an instance of RNN\r\n",
        "rnn = RNN2(EMBEDDING_DIM, HIDDEN_DIM, vocab_size(), output_size)\r\n",
        "\r\n",
        "print(rnn)"
      ],
      "outputs": [],
      "metadata": {
        "id": "molvr74MSwlD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def train(data, epochs, print_every):\r\n",
        "  for i_epoch in range(epochs):\r\n",
        "      if not i_epoch % print_every:\r\n",
        "          print(\"Finnished epoch \" + str(i_epoch / 30 * 100)  + \"%\")\r\n",
        "      \r\n",
        "      for i_count, row in data.iterrows():\r\n",
        "          sentence = train_tweets[i]\r\n",
        "          sent_class = tweet_sent_class[i]\r\n",
        "          # Step 1. Remember that Pytorch accumulates gradients.\r\n",
        "                  # We need to clear them out before each instance\r\n",
        "          # Also, we need to clear out the hidden state of the LSTM,\r\n",
        "                  # detaching it from its history on the last instance.\r\n",
        "          hidden = rnn.init_hidden()\r\n",
        "          rnn.zero_grad()\r\n",
        "          \r\n",
        "          # Step 2. Get our inputs ready for the network, that is, turn them into\r\n",
        "          # Tensors of word indices.\r\n",
        "          sentence_in = prepare_sequence(sentence)\r\n",
        "          target_class = map_class(sent_class)\r\n",
        "\r\n",
        "          # Step 3. Run our forward pass.\r\n",
        "          for i in range(len(sentence_in)):\r\n",
        "              class_scores, hidden = rnn(sentence_in[i], hidden)\r\n",
        "\r\n",
        "          # Step 4. Compute the loss, gradients, and update the parameters by\r\n",
        "          #  calling optimizer.step()\r\n",
        "          loss = loss_function(class_scores, target_class)\r\n",
        "          loss.backward()\r\n",
        "          optimizer.step()"
      ],
      "outputs": [],
      "metadata": {
        "id": "EkxJbHWhS3U_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Exemplo de https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/\r\n",
        "class SentenceClassifier(nn.Module):\r\n",
        "    \r\n",
        "    #define all the layers used in model\r\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \r\n",
        "                 bidirectional, dropout, n_batch_size):\r\n",
        "        \r\n",
        "        #Constructor\r\n",
        "        super().__init__()          \r\n",
        "        \r\n",
        "        #embedding layer\r\n",
        "        #self.embedding = nn.Embedding(vocab_size, embedding_dim)\r\n",
        "        \r\n",
        "        #lstm layer\r\n",
        "        self.lstm = nn.LSTM(vocab_size, #embedding_dim, \r\n",
        "                           hidden_dim, \r\n",
        "                           num_layers=n_layers, \r\n",
        "                           bidirectional=bidirectional, \r\n",
        "                           dropout=dropout,\r\n",
        "                           batch_first=True)\r\n",
        "        \r\n",
        "        #dense layer\r\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\r\n",
        "        \r\n",
        "        #activation function\r\n",
        "        #self.act = nn.ReLU()\r\n",
        "        self.act = nn.Sigmoid()\r\n",
        "        \r\n",
        "    def forward(self, word_vector): #text, text_lengths):\r\n",
        "        \r\n",
        "        #text = [batch size,sent_length]\r\n",
        "        #embedded = self.embedding(text)\r\n",
        "        #embedded = [batch size, sent_len, emb dim]\r\n",
        "      \r\n",
        "        #packed sequence\r\n",
        "        #packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\r\n",
        "        #packed_words = nn.utils.rnn.pack_padded_sequence(word_vector, 1, batch_first=True)\r\n",
        "        \r\n",
        "        packed_output, (hidden, cell) = self.lstm(word_vector) #packed_embedded)\r\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\r\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\r\n",
        "        \r\n",
        "        #concat the final forward and backward hidden state\r\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\r\n",
        "                \r\n",
        "        #hidden = [batch size, hid dim * num directions]\r\n",
        "        dense_outputs=self.fc(hidden)\r\n",
        "\r\n",
        "        #Final activation function\r\n",
        "        outputs=self.act(dense_outputs)\r\n",
        "        \r\n",
        "        return outputs\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# Exemplo de https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/\r\n",
        "class SentenceClassifierEmb(nn.Module):\r\n",
        "    \r\n",
        "    #define all the layers used in model\r\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \r\n",
        "                 bidirectional, dropout, n_batch_size):\r\n",
        "        \r\n",
        "        #Constructor\r\n",
        "        super().__init__()          \r\n",
        "        \r\n",
        "        #embedding layer\r\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\r\n",
        "        \r\n",
        "        #lstm layer\r\n",
        "        self.lstm = nn.LSTM(embedding_dim, \r\n",
        "                           hidden_dim, \r\n",
        "                           num_layers=n_layers, \r\n",
        "                           bidirectional=bidirectional, \r\n",
        "                           dropout=dropout,\r\n",
        "                           batch_first=True)\r\n",
        "        \r\n",
        "        #dense layer\r\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\r\n",
        "        \r\n",
        "        #activation function\r\n",
        "        #self.act = nn.ReLU()\r\n",
        "        self.act = nn.Sigmoid()\r\n",
        "        \r\n",
        "    def forward(self, word_vector): #text, text_lengths):\r\n",
        "        \r\n",
        "        #text = [batch size,sent_length]\r\n",
        "        embedded = self.embedding(word_vector)\r\n",
        "        #embedded = [batch size, sent_len, emb dim]\r\n",
        "      \r\n",
        "        #packed sequence\r\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, batch_first=True)\r\n",
        "        #packed_words = nn.utils.rnn.pack_padded_sequence(word_vector, 1, batch_first=True)\r\n",
        "        \r\n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\r\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\r\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\r\n",
        "        \r\n",
        "        #concat the final forward and backward hidden state\r\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\r\n",
        "                \r\n",
        "        #hidden = [batch size, hid dim * num directions]\r\n",
        "        dense_outputs=self.fc(hidden)\r\n",
        "\r\n",
        "        #Final activation function\r\n",
        "        outputs=self.act(dense_outputs)\r\n",
        "        \r\n",
        "        return outputs"
      ],
      "outputs": [],
      "metadata": {
        "id": "g5rLheCoWJLk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#define hyperparameters\r\n",
        "size_of_vocab = vocab_size()\r\n",
        "embedding_dim = len(w2v_model.wv.vectors[0])\r\n",
        "num_hidden_nodes = 255\r\n",
        "num_output_nodes = 1\r\n",
        "num_layers = 2\r\n",
        "bidirection = True\r\n",
        "dropout = 0.2\r\n",
        "batch_size = 200\r\n",
        "\r\n",
        "#instantiate the model\r\n",
        "model = SentenceClassifierEmb(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, \r\n",
        "                   bidirectional = True, dropout = dropout, n_batch_size = batch_size)\r\n",
        "\r\n",
        "#architecture\r\n",
        "print(model)\r\n",
        "\r\n",
        "#No. of trianable parameters\r\n",
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "    \r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\r\n",
        "\r\n",
        "#define optimizer and loss\r\n",
        "optimizer = torch.optim.Adam(model.parameters())\r\n",
        "criterion = nn.BCELoss()\r\n",
        "\r\n",
        "#define metric\r\n",
        "def binary_accuracy(preds, y):\r\n",
        "    #round predictions to the closest integer\r\n",
        "    rounded_preds = torch.round(preds)\r\n",
        "    print(preds, y, rounded_preds)\r\n",
        "    \r\n",
        "    correct = (rounded_preds == y).float() \r\n",
        "    acc = correct.sum() / len(correct)\r\n",
        "    return acc\r\n",
        "\r\n",
        "def get_batches(iterator, batch_count):\r\n",
        "    counter = 0\r\n",
        "    total_count = 0\r\n",
        "    x = np.array([])\r\n",
        "    y = np.array([])\r\n",
        "    for index, row in iterator:\r\n",
        "      #print(row)\r\n",
        "      counter = counter + 1\r\n",
        "      total_count = total_count + 1\r\n",
        "      #print((x, row.preprocessed_words))\r\n",
        "      #x =  np.concatenate((x, to_x(row.preprocessed_words)))\r\n",
        "      x =  np.concatenate((x, row.preprocessed_words))\r\n",
        "      #print((y, row.sentiment))\r\n",
        "      y = np.concatenate((y, to_y(row.sentiment)))\r\n",
        "      #print(row.sentiment, y)\r\n",
        "      \r\n",
        "      while counter >= batch_count:\r\n",
        "        yield x, torch.from_numpy(y).float(), counter, total_count\r\n",
        "        #yield torch.from_numpy(x).float(), torch.from_numpy(y).float(), counter, total_count\r\n",
        "        counter = 0\r\n",
        "        x = np.array([])\r\n",
        "        y = np.array([])\r\n",
        "\r\n",
        "    if counter > 0:\r\n",
        "      yield x, torch.from_numpy(y).float(), counter, total_count\r\n",
        "      #yield torch.from_numpy(x).float(), torch.from_numpy(y).float(), counter, total_count\r\n",
        "    \r\n",
        "def train(model, dataset, optimizer, criterion, n_batch_count=20):\r\n",
        "    \r\n",
        "    #initialize every epoch \r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_acc = 0\r\n",
        "    \r\n",
        "    #set the model in training phase\r\n",
        "    model.train()  \r\n",
        "    \r\n",
        "    import torchtext\r\n",
        "\r\n",
        "    for batch_x, batch_y, batch_count, total_count in get_batches(dataset.iterrows(), n_batch_count):\r\n",
        "    #for batch in torchtext.data.BucketIterator(iterator, n_batch_count):\r\n",
        "    #for batch in data.BucketIterator.splits(iterator, n_batch_count):\r\n",
        "    \r\n",
        "\r\n",
        "        #resets the gradients after every batch\r\n",
        "        optimizer.zero_grad()   \r\n",
        "        \r\n",
        "        #retrieve text and no. of words\r\n",
        "\r\n",
        "        #x = to_x(row.preprocessed_words).float()\r\n",
        "        #print(x)\r\n",
        "        #y = to_y(row.sentiment).float()\r\n",
        "        \r\n",
        "        # [samples,time steps,features]\r\n",
        "        x = batch_x #.reshape(batch_count, 1, vocab_size())\r\n",
        "        #x = batch_x.reshape(batch_count, 1, vocab_size())\r\n",
        "        y = batch_y\r\n",
        "        #print(x.shape, y.shape)\r\n",
        "        #print(x)\r\n",
        "\r\n",
        "        #convert to 1D tensor\r\n",
        "        predictions = model(x).squeeze()\r\n",
        "        #print(predictions.shape)\r\n",
        "        #print(predictions, y)\r\n",
        "        \r\n",
        "        #compute the loss\r\n",
        "        loss = criterion(predictions, y)\r\n",
        "        \r\n",
        "        #compute the binary accuracy\r\n",
        "        acc = binary_accuracy(predictions, y)\r\n",
        "        \r\n",
        "        #backpropage the loss and compute the gradients\r\n",
        "        loss.backward()       \r\n",
        "        \r\n",
        "        #update the weights\r\n",
        "        optimizer.step()      \r\n",
        "        \r\n",
        "        #loss and accuracy\r\n",
        "        epoch_loss += loss.item()  \r\n",
        "        epoch_acc += acc.item()\r\n",
        "\r\n",
        "        print('batch', total_count, epoch_loss, epoch_acc)\r\n",
        "\r\n",
        "    return epoch_loss / total_count, epoch_acc / total_count\r\n",
        "\r\n",
        "\r\n",
        "N_EPOCHS = 5\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "     \r\n",
        "    #train the model\r\n",
        "    train_loss, train_acc = train(model, train_df, optimizer, criterion, batch_size)\r\n",
        "\r\n",
        "    #evaluate the model\r\n",
        "    #valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\r\n",
        "    \r\n",
        "    #save the best model\r\n",
        "    #if valid_loss < best_valid_loss:\r\n",
        "    #    best_valid_loss = valid_loss\r\n",
        "    #    torch.save(model.state_dict(), 'saved_weights.pt')\r\n",
        "    \r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\r\n",
        "    #print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "outputs": [],
      "metadata": {
        "id": "eb0Nl-0RWZ1Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "wYrgLomQoOg2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Visualizing Word2Vec Embeddings with t-SNE\r\n",
        "from sklearn.manifold import TSNE\r\n",
        "import random\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "rnn = WordRNN()\r\n",
        "\r\n",
        "n_samples = 500\r\n",
        "# Sample random words from model dictionary\r\n",
        "random_i = random.sample(range(vocab_size), n_samples)\r\n",
        "random_w = [rnn.token2word(i) for i in random_i]\r\n",
        "\r\n",
        "# Generate Word2Vec embeddings of each word\r\n",
        "word_vecs = np.array([w2v_model[w] for w in random_w])\r\n",
        "\r\n",
        "# Apply t-SNE to Word2Vec embeddings, reducing to 2 dims\r\n",
        "tsne = TSNE()\r\n",
        "tsne_e = tsne.fit_transform(word_vecs)\r\n",
        "\r\n",
        "# Plot t-SNE result\r\n",
        "plt.figure(figsize=(32, 32))\r\n",
        "plt.scatter(tsne_e[:, 0], tsne_e[:, 1], marker='o', c=range(len(random_w)), cmap=plt.get_cmap('Spectral'))\r\n",
        "\r\n",
        "for label, x, y, in zip(random_w, tsne_e[:, 0], tsne_e[:, 1]):\r\n",
        "    plt.annotate(label,\r\n",
        "                 xy=(x, y), xytext=(0, 15),\r\n",
        "                 textcoords='offset points', ha='right', va='bottom',\r\n",
        "                 bbox=dict(boxstyle='round, pad=0.2', fc='yellow', alpha=0.1))"
      ],
      "outputs": [],
      "metadata": {
        "id": "F6uFPDi1veBR"
      }
    }
  ]
}