{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.3 64-bit ('venv': virtualenv)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Deep Learning - Trabalho Final - Classificação de textos para análise de sentimentos",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "4082b515cb93b7843f5ff6f6ad97f95054605209e213f5ac575cc7757f0acb1a"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Classificação de textos para análise de sentimentos\r\n",
        "\r\n",
        "Trabalho final da disciplina de deep learning da pós graduação em data science da FURB. \r\n",
        "Professor: @luann.porfirio\r\n",
        "Aluno: João Poffo\r\n",
        "\r\n",
        "# Instruções do professor\r\n",
        "\r\n",
        "Base de dados \r\n",
        "\r\n",
        "Instruções:\r\n",
        "- O objetivo deste trabalho é criar um modelo binário de aprendizado de máquina para classificação de textos. \r\n",
        "Para isso, será utilizado a base de dados [IMDb](http://ai.stanford.edu/~amaas/data/sentiment/), que consiste de dados textuais de críticas positivas e negativas de filmes\r\n",
        "- Uma vez treinado, o modelo deve ter uma função `predict` que recebe uma string como parâmetro e retorna o valor 1 ou 0, aonde 1 significa uma crítica positiva e 0 uma crítica negativa\r\n",
        "- O pré-processamento pode ser desenvolvido conforme desejar (ex.: remoção de stopwords, word embedding, one-hot encoding, char encoding)\r\n",
        "- É preferível que seja empregado um modelo de recorrência (ex.: rnn, lstm, gru) para a etapa de classificação\r\n",
        "- Documente o código (explique sucintamente o que cada função faz, insira comentários em trechos de código relevantes)\r\n",
        "- **Atenção**: Uma vez treinado o modelo final, salve-o no diretório do seu projeto e crie uma célula ao final do notebook contendo uma função de leitura deste arquivo, juntamente com a execução da função `predict`\r\n",
        "\r\n",
        "Sugestões:\r\n",
        "- Explorar a base de dados nas células iniciais do notebook para ter um melhor entendimento do problema, distribuição dos dados, etc\r\n",
        "- Após desenvolver a estrutura de classificação, é indicado fazer uma busca de hiperparâmetros e comparar os resultados obtidos em diferentes situações\r\n",
        "\r\n",
        "Prazo de entrega:\r\n",
        "- 01-08-2021 às 23:59hs GMT-3\r\n",
        "\r\n",
        "Formato preferível de entrega:\r\n",
        "- Postar no portal Ava da disciplina o link do projeto no github (ou anexar o projeto diretamente no portal Ava)\r\n",
        "\r\n",
        "luann.porfirio@gmail.com"
      ],
      "metadata": {
        "id": "DK_cKZf9Dv4n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "source": [
        "# Instalando libs\r\n",
        "!pip install torchtext\r\n",
        "!pip install gensim\r\n",
        "!pip install pandas\r\n",
        "!pip install sklearn\r\n",
        "\r\n",
        "#Necessário se usássemos o vocabulário pré-treinado glove\r\n",
        "#!pip install spacy\r\n",
        "#!python -m spacy download en_core_web_sm"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://joao.poffo%40ambevtech.com.br:****@pkgs.dev.azure.com/AMBEV-SA/AMBEV-BIFROST/_packaging/canaa-packages/pypi/simple/\n",
            "Requirement already satisfied: torchtext in .\\venv\\lib\\site-packages (0.10.0)\n",
            "Requirement already satisfied: torch==1.9.0 in .\\venv\\lib\\site-packages (from torchtext) (1.9.0)\n",
            "Requirement already satisfied: requests in .\\venv\\lib\\site-packages (from torchtext) (2.26.0)\n",
            "Requirement already satisfied: tqdm in .\\venv\\lib\\site-packages (from torchtext) (4.62.0)\n",
            "Requirement already satisfied: numpy in .\\venv\\lib\\site-packages (from torchtext) (1.21.1)\n",
            "Requirement already satisfied: typing-extensions in .\\venv\\lib\\site-packages (from torch==1.9.0->torchtext) (3.10.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in .\\venv\\lib\\site-packages (from requests->torchtext) (2021.5.30)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in .\\venv\\lib\\site-packages (from requests->torchtext) (1.26.6)\n",
            "Requirement already satisfied: idna<4,>=2.5 in .\\venv\\lib\\site-packages (from requests->torchtext) (3.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in .\\venv\\lib\\site-packages (from requests->torchtext) (2.0.4)\n",
            "Requirement already satisfied: colorama in .\\venv\\lib\\site-packages (from tqdm->torchtext) (0.4.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://joao.poffo%40ambevtech.com.br:****@pkgs.dev.azure.com/AMBEV-SA/AMBEV-BIFROST/_packaging/canaa-packages/pypi/simple/\n",
            "Requirement already satisfied: gensim in .\\venv\\lib\\site-packages (4.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in .\\venv\\lib\\site-packages (from gensim) (1.21.1)\n",
            "Requirement already satisfied: Cython==0.29.21 in .\\venv\\lib\\site-packages (from gensim) (0.29.21)\n",
            "Requirement already satisfied: scipy>=0.18.1 in .\\venv\\lib\\site-packages (from gensim) (1.7.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in .\\venv\\lib\\site-packages (from gensim) (5.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://joao.poffo%40ambevtech.com.br:****@pkgs.dev.azure.com/AMBEV-SA/AMBEV-BIFROST/_packaging/canaa-packages/pypi/simple/\n",
            "Requirement already satisfied: pandas in .\\venv\\lib\\site-packages (1.3.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in .\\venv\\lib\\site-packages (from pandas) (2021.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in .\\venv\\lib\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in .\\venv\\lib\\site-packages (from pandas) (1.21.1)\n",
            "Requirement already satisfied: six>=1.5 in .\\venv\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://joao.poffo%40ambevtech.com.br:****@pkgs.dev.azure.com/AMBEV-SA/AMBEV-BIFROST/_packaging/canaa-packages/pypi/simple/\n",
            "Requirement already satisfied: sklearn in .\\venv\\lib\\site-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in .\\venv\\lib\\site-packages (from sklearn) (0.24.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in .\\venv\\lib\\site-packages (from scikit-learn->sklearn) (1.21.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in .\\venv\\lib\\site-packages (from scikit-learn->sklearn) (2.2.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in .\\venv\\lib\\site-packages (from scikit-learn->sklearn) (1.7.0)\n",
            "Requirement already satisfied: joblib>=0.11 in .\\venv\\lib\\site-packages (from scikit-learn->sklearn) (1.0.1)\n"
          ]
        }
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-10T00:36:57.559764Z",
          "start_time": "2021-06-10T00:36:52.638020Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXoNI6TsDv4x",
        "outputId": "33f11002-55cb-4fb0-dd50-8198553a3c4b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "source": [
        "# Bibliotecas necessárias\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "from torchtext.legacy import datasets\r\n",
        "from torchtext.legacy import data\r\n",
        "\r\n",
        "import random\r\n",
        "import pandas\r\n",
        "import gensim\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "import time\r\n"
      ],
      "outputs": [],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-10T00:44:12.514627Z",
          "start_time": "2021-06-10T00:44:12.509125Z"
        },
        "id": "NAsaMG8VDv41"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "source": [
        "# PREPARAÇÃO DOS DADOS\r\n",
        "# Primeiro teste foi usando TFIDF, mas nem coloquei pq ficou muito ruim e, pensando bem, não faz sentido. \r\n",
        "#  Pq aqui não queremos palavras \"importantes\" para definir um documento, mas sim, classificá-los com o sentimento. \r\n",
        "\r\n",
        "# Em seguida usamos o gensim para tokenização por causa do recurso de stemming.\r\n",
        "\r\n",
        "# Exemplo com pytorch (muito didático!): https://github.com/bentrevett/pytorch-sentiment-analysis\r\n",
        "# Exemplo do gensim: https://rohit-agrawal.medium.com/using-fine-tuned-gensim-word2vec-embeddings-with-torchtext-and-pytorch-17eea2883cd\r\n",
        "\r\n",
        "# Seed para que os testes possam ser reproduzidos. Em produção, é melhor e mais rápido desligar ambos.\r\n",
        "SEED = 789\r\n",
        "torch.manual_seed(SEED)\r\n",
        "\r\n",
        "# Não precisa ser determinístico pq é a mesma versão e máquina que estamos comparando.\r\n",
        "#torch.backends.cudnn.deterministic = True\r\n",
        "\r\n",
        "# Definição dos filtros padrão do gensim. Estamos deixo abaixo para documentar a ordem.\r\n",
        "#gensim.parsing.preprocessing.DEFAULT_FILTERS = [\r\n",
        "#    lambda x: x.lower(), strip_tags, strip_punctuation,\r\n",
        "#    strip_multiple_whitespaces, strip_numeric,\r\n",
        "#    remove_stopwords, strip_short, stem_text\r\n",
        "#]\r\n",
        "\r\n",
        "# Função de tokenização\r\n",
        "def tokenize(sentence):\r\n",
        "    return gensim.parsing.preprocessing.preprocess_string(sentence)\r\n",
        "\r\n",
        "# Montando o tipo dos campos do torch\r\n",
        "\r\n",
        "# Aqui que associamos o tipo do campo com o gensim para construirmos o vocábulário adiante\r\n",
        "TEXT = data.Field(tokenize=tokenize)\r\n",
        "LABEL = data.LabelField(dtype = torch.float)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "pszVCeNEYaue"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "source": [
        "# Função para buscar os dados e quebrar em treino, teste e validação\r\n",
        "# - Necessário pois fazemos novos testes adiante.\r\n",
        "def train_test_valid():\r\n",
        "\r\n",
        "    # Trabalhando com o IMDb\r\n",
        "    train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\r\n",
        "\r\n",
        "    # Separa os dados de treino em treino e validação em 70/30\r\n",
        "    train_data, valid_data = train_data.split(random_state = random.seed(SEED))\r\n",
        "\r\n",
        "    return train_data, test_data, valid_data\r\n",
        "\r\n",
        "# Obtém os dados e faz o split\r\n",
        "train_data, test_data, valid_data = train_test_valid()\r\n",
        "\r\n",
        "train_data\r\n"
      ],
      "outputs": [],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-10T00:50:27.424355Z",
          "start_time": "2021-06-10T00:49:16.448387Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSTFeBZ-Dv42",
        "outputId": "2aed927a-71f1-4210-8e95-0adbf60cbf96"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "source": [
        "# Verificação do split dos dados\r\n",
        "print('Exemplos de treino (35%):', len(train_data), ', teste (50%):', len(test_data), ' e validação (15%):', len(valid_data))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exemplos de treino (35%): 17500 , teste (50%): 25000  e validação (15%): 7500\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCziwLteZxSP",
        "outputId": "13e3da21-99df-45c7-99ac-66e93390d3a4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "source": [
        "# Teste com vocabulário de 30 mil palavras\r\n",
        "MAX_VOCAB_SIZE = 30000\r\n",
        "\r\n",
        "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\r\n",
        "LABEL.build_vocab(train_data)\r\n",
        "\r\n",
        "print(\"Palavras únicas:\", len(TEXT.vocab))\r\n",
        "print(\"Classes únicas:\", len(LABEL.vocab))\r\n",
        "print(\"20 palavras mais comuns:\", TEXT.vocab.freqs.most_common(20))\r\n",
        "print(\"10 palavras do vocabulário de palavras para analisar estrutura:\", TEXT.vocab.itos[:10])\r\n",
        "print(\"Estrutura das classes:\", LABEL.vocab.stoi)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palavras únicas: 30002\n",
            "Classes únicas: 2\n",
            "20 palavras mais comuns: [('movi', 36384), ('film', 34043), ('like', 16064), ('time', 11242), ('good', 10752), ('charact', 9860), ('watch', 9781), ('stori', 9233), ('scene', 7433), ('look', 7114), ('end', 6838), ('bad', 6587), ('peopl', 6489), ('great', 6383), ('love', 6285), ('think', 6262), ('wai', 6192), ('act', 6183), ('plai', 6165), ('thing', 5778)]\n",
            "10 palavras do vocabulário de palavras para analisar estrutura: ['<unk>', '<pad>', 'movi', 'film', 'like', 'time', 'good', 'charact', 'watch', 'stori']\n",
            "Estrutura das classes: defaultdict(None, {'neg': 0, 'pos': 1})\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klynVEagdHQA",
        "outputId": "53473727-9c95-4d45-f9a9-c741d29cf254"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "source": [
        "# Criando lotes de 64 através do iterador para o treino\r\n",
        "BATCH_SIZE = 64\r\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\r\n",
        "    (train_data, valid_data, test_data), \r\n",
        "    batch_size = BATCH_SIZE)\r\n",
        "\r\n",
        "# Classe de rede neural recorrente padrão simples\r\n",
        "class RNN(nn.Module):\r\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\r\n",
        "        \r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        # Usamos o embedding pois é uma camada pra transformar um vetor esparço de dicionário em um denso. \r\n",
        "        # Na teoria, palavras com impacto similar na classificação dos sentimentos são mapeadas próximas uma das outras.\r\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\r\n",
        "        \r\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\r\n",
        "        \r\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\r\n",
        "        \r\n",
        "    def forward(self, text):\r\n",
        "\r\n",
        "        #text = [sent len, batch size]\r\n",
        "        embedded = self.embedding(text)\r\n",
        "        \r\n",
        "        #embedded = [sent len, batch size, emb dim]\r\n",
        "        output, hidden = self.rnn(embedded)\r\n",
        "        \r\n",
        "        #output = [sent len, batch size, hid dim]\r\n",
        "        #hidden = [1, batch size, hid dim]        \r\n",
        "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\r\n",
        "        \r\n",
        "        return self.fc(hidden.squeeze(0))\r\n",
        "\r\n",
        "# A quantidade features é a quantidade de palavras no nosso dicionário\r\n",
        "INPUT_DIM = len(TEXT.vocab)\r\n",
        "\r\n",
        "# Quão denso deve ser nosso embeeding\r\n",
        "EMBEDDING_DIM = 100\r\n",
        "\r\n",
        "# Tamanho da cama oculta\r\n",
        "HIDDEN_DIM = 256\r\n",
        "\r\n",
        "# Tamanho da saída\r\n",
        "OUTPUT_DIM = 1\r\n",
        "\r\n",
        "# Cria o modelo com os hiperparâmetros definidos acima\r\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\r\n",
        "\r\n",
        "# Imprime a arquitetura\r\n",
        "print(model)\r\n",
        "\r\n",
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "print(f'Esta modelo tem {count_parameters(model):,} parâmetros treináveis.')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN(\n",
            "  (embedding): Embedding(30002, 100)\n",
            "  (rnn): RNN(100, 256)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            ")\n",
            "Esta modelo tem 3,092,105 parâmetros treináveis.\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQBjIsDcff2T",
        "outputId": "350932a4-682b-440a-d7e5-41e68f7c160d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "source": [
        "# No primeiro teste vamos usar um otimizador simples - gradiente descendente estocástico\r\n",
        "# - learning_rate = 0.001\r\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)\r\n",
        "\r\n",
        "# Como loss usamos este do torch que traz tanto a sigmoide quanto a entropia cruzada binária.\r\n",
        "criterion = nn.BCEWithLogitsLoss()\r\n",
        "\r\n",
        "# Função para calcular acurácia binária. \r\n",
        "def binary_accuracy(preds, y):\r\n",
        "    \"\"\"\r\n",
        "    Retorna a acurácia no lote. Por exemplo, 6/10 corretas, retorna 0.6.\r\n",
        "    Ao mesmo tempo traduz a predição para uma das classes binárias disponíveis através de arredondamento. Por exemplo: 0.6 -> 1 (verdadeiro), 0.3 -> 0 (falso).\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\r\n",
        "    correct = (rounded_preds == y).float()\r\n",
        "    acc = correct.sum() / len(correct)\r\n",
        "    return acc\r\n",
        "\r\n",
        "# Função de treino recebendo o modelo, os dados, o otimizador e a loss como argumentos\r\n",
        "def train(model, iterator, optimizer, criterion):\r\n",
        "    \r\n",
        "    # loss e acurácia acumulada\r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_acc = 0\r\n",
        "    \r\n",
        "    # Seta o modelo para o modo de treino\r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    # Para cada lote de dados\r\n",
        "    for batch in iterator:\r\n",
        "        \r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        predictions = model(batch.text).squeeze(1)\r\n",
        "        \r\n",
        "        loss = criterion(predictions, batch.label)\r\n",
        "        \r\n",
        "        acc = binary_accuracy(predictions, batch.label)\r\n",
        "        \r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        epoch_loss += loss.item()\r\n",
        "        epoch_acc += acc.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\r\n",
        "\r\n",
        "# Função de validação\r\n",
        "def evaluate(model, iterator, criterion):\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_acc = 0\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for batch in iterator:\r\n",
        "\r\n",
        "            predictions = model(batch.text).squeeze(1)\r\n",
        "            \r\n",
        "            loss = criterion(predictions, batch.label)\r\n",
        "            \r\n",
        "            acc = binary_accuracy(predictions, batch.label)\r\n",
        "\r\n",
        "            epoch_loss += loss.item()\r\n",
        "            epoch_acc += acc.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\r\n",
        "\r\n",
        "# Função para ajudar no cálculo de cada época\r\n",
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs\r\n",
        "\r\n",
        "# Quantidade de épocas usadas no treino\r\n",
        "N_EPOCHS = 5\r\n",
        "\r\n",
        "# Treino está como um processo pois é padrão\r\n",
        "def process(model, train_fnc=train, evaluate_fnc=evaluate, save_name='tf-model1'):\r\n",
        "    # Ajuda a identificar a melhor época para salvar o modelo\r\n",
        "    best_valid_loss = float('inf')\r\n",
        "\r\n",
        "    for epoch in range(N_EPOCHS):\r\n",
        "\r\n",
        "        start_time = time.time()\r\n",
        "        \r\n",
        "        train_loss, train_acc = train_fnc(model, train_iterator, optimizer, criterion)\r\n",
        "        valid_loss, valid_acc = evaluate_fnc(model, valid_iterator, criterion)\r\n",
        "        \r\n",
        "        end_time = time.time()\r\n",
        "\r\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "        \r\n",
        "        if valid_loss < best_valid_loss:\r\n",
        "            best_valid_loss = valid_loss\r\n",
        "            # Full - Não precisa de hiperparâmetros\r\n",
        "            torch.save(model, save_name + '.ptf')\r\n",
        "            # State - Precisa inicializar a classe com os mesmo hiperparâmetros\r\n",
        "            torch.save(model.state_dict(), save_name + '.pt')\r\n",
        "        \r\n",
        "        print(f'Época: {epoch+1:02} | tempo de processamento: {epoch_mins}m {epoch_secs}s')\r\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\r\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\r\n",
        "\r\n",
        "# Executa o treino do modelo\r\n",
        "process(model)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 01 | tempo de processamento: 5m 53s\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.87%\n",
            "\t Val. Loss: 0.697 |  Val. Acc: 50.83%\n",
            "Época: 02 | tempo de processamento: 6m 14s\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.96%\n",
            "\t Val. Loss: 0.697 |  Val. Acc: 49.51%\n",
            "Época: 03 | tempo de processamento: 6m 11s\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.81%\n",
            "\t Val. Loss: 0.697 |  Val. Acc: 49.63%\n",
            "Época: 04 | tempo de processamento: 6m 6s\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.73%\n",
            "\t Val. Loss: 0.697 |  Val. Acc: 49.60%\n",
            "Época: 05 | tempo de processamento: 6m 11s\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.17%\n",
            "\t Val. Loss: 0.697 |  Val. Acc: 49.54%\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2IBFNdagLdq",
        "outputId": "5a91fe49-cff9-4b0e-b14f-ee0a6b624f42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análise do primeiro algoritmo\r\n",
        "\r\n",
        "Acurácia muito ruim (aprox. 50%) com a RNN simples com as N palavras mais frequentes.\r\n",
        "\r\n",
        "Vamos testar um modelo usando uma rede recorrente um pouco mais complexa.\r\n",
        "\r\n",
        "RNN sofre de um problema de perda de gradiente. Essa perda de gradiente faz com que a ordem das palavras perdam relevância no resultado da predição. \r\n",
        "\r\n",
        "LSTM (Long Short-Term Memory) nos ajuda resolver isso tendo uma camada extra de memória que se chama célula/cell.\r\n",
        "\r\n",
        "Adicionamos também uma camada de regularização com dropout que \"desativa\" certos neurônios em uma iteração para que reduza bias e vício de caminhos melhorando a acertividade do modelo e criando um tipo de aleatoriedade que existe no processamento de linguagem natural."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Apenas a classe separada pois é a única coisa necessária declarar para a predição (no final do notebook).\r\n",
        "class MultiLayerRNN(nn.Module):\r\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \r\n",
        "                 bidirectional, dropout, pad_idx):\r\n",
        "        \r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\r\n",
        "        \r\n",
        "        # Camada do LSTM\r\n",
        "        self.rnn = nn.LSTM(embedding_dim, \r\n",
        "                           hidden_dim, \r\n",
        "                           num_layers=n_layers, \r\n",
        "                           bidirectional=bidirectional, \r\n",
        "                           dropout=dropout)\r\n",
        "        \r\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, text, text_lengths):\r\n",
        "        \r\n",
        "        #text = [sent len, batch size]\r\n",
        "        embedded = self.dropout(self.embedding(text))\r\n",
        "        \r\n",
        "        #embedded = [sent len, batch size, emb dim]\r\n",
        "        \r\n",
        "        #Empacota a sequência (remove paddings) e somente processa isso. Porém, retorna empacotado também.\r\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\r\n",
        "        \r\n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\r\n",
        "        \r\n",
        "        #Então, desempacota\r\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\r\n",
        "\r\n",
        "        #output = [sent len, batch size, hid dim * num directions]\r\n",
        "        #output over padding tokens are zero tensors\r\n",
        "        \r\n",
        "        #hidden = [num layers * num directions, batch size, hid dim]\r\n",
        "        #cell = [num layers * num directions, batch size, hid dim]\r\n",
        "        \r\n",
        "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\r\n",
        "        #and apply dropout\r\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\r\n",
        "                \r\n",
        "        #hidden = [batch size, hid dim * num directions]\r\n",
        "            \r\n",
        "        return self.fc(hidden)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "source": [
        "# Precisamos criar uma nova função de treino pq agora tem os tamanhos também\r\n",
        "def train_with_sequences(model, iterator, optimizer, criterion):\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_acc = 0\r\n",
        "    \r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    for batch in iterator:\r\n",
        "        \r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        text, text_lengths = batch.text\r\n",
        "        \r\n",
        "        predictions = model(text, text_lengths).squeeze(1)\r\n",
        "        \r\n",
        "        loss = criterion(predictions, batch.label)\r\n",
        "        \r\n",
        "        acc = binary_accuracy(predictions, batch.label)\r\n",
        "        \r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        epoch_loss += loss.item()\r\n",
        "        epoch_acc += acc.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\r\n",
        "\r\n",
        "# Mesma necessidade do treino.\r\n",
        "def evaluate_with_sequences(model, iterator, criterion):\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_acc = 0\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for batch in iterator:\r\n",
        "\r\n",
        "            text, text_lengths = batch.text\r\n",
        "            \r\n",
        "            predictions = model(text, text_lengths).squeeze(1)\r\n",
        "            \r\n",
        "            loss = criterion(predictions, batch.label)\r\n",
        "            \r\n",
        "            acc = binary_accuracy(predictions, batch.label)\r\n",
        "\r\n",
        "            epoch_loss += loss.item()\r\n",
        "            epoch_acc += acc.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\r\n",
        "\r\n",
        "# Precisamos incluir o tamanho das sentenças no vocabulário (include_length=true).\r\n",
        "TEXT = data.Field(tokenize = tokenize,\r\n",
        "                  include_lengths = True)\r\n",
        "\r\n",
        "# Precisa recarregar dados\r\n",
        "train_data, test_data, valid_data = train_test_valid()\r\n",
        "\r\n",
        "TEXT.build_vocab(train_data, \r\n",
        "    max_size = MAX_VOCAB_SIZE, \r\n",
        "    # Teste com o glove ficaria pra depois\r\n",
        "    #vectors = \"glove.6B.100d\", \r\n",
        "    #unk_init = torch.Tensor.normal_\r\n",
        "    )\r\n",
        "LABEL.build_vocab(train_data)\r\n",
        "\r\n",
        "# Comentado abaixo pois mantém do último treino\r\n",
        "#INPUT_DIM = len(TEXT.vocab)\r\n",
        "#EMBEDDING_DIM = 100\r\n",
        "#HIDDEN_DIM = 256\r\n",
        "#OUTPUT_DIM = 1\r\n",
        "\r\n",
        "# Novos hiperparâmetros\r\n",
        "N_LAYERS = 2\r\n",
        "BIDIRECTIONAL = True\r\n",
        "DROPOUT = 0.5\r\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\r\n",
        "\r\n",
        "# Cria o modelo com LSTM e hiperparâmetros\r\n",
        "model2 = MultiLayerRNN(INPUT_DIM, \r\n",
        "            EMBEDDING_DIM, \r\n",
        "            HIDDEN_DIM, \r\n",
        "            OUTPUT_DIM, \r\n",
        "            N_LAYERS, \r\n",
        "            BIDIRECTIONAL, \r\n",
        "            DROPOUT, \r\n",
        "            PAD_IDX)\r\n",
        "\r\n",
        "# Rotinas de reuso do vocabulário treinado. Comentado pois não utilizamos.\r\n",
        "#pretrained_embeddings = TEXT.vocab.vectors\r\n",
        "#print(pretrained_embeddings.shape)\r\n",
        "#model2.embedding.weight.data.copy_(pretrained_embeddings)\r\n",
        "#UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\r\n",
        "#model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\r\n",
        "#model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\r\n",
        "#print(model.embedding.weight.data)\r\n",
        "\r\n",
        "# Mudamos para um otimizador mais eficiente: Adam\r\n",
        "optimizer = optim.Adam(model2.parameters())\r\n",
        "\r\n",
        "# Precisa recriar os iteradores.\r\n",
        "# - Outro detalhe necessário é ordenar por tamanho. O iterador já faz isso usando o sort_within_batch = True.\r\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\r\n",
        "    (train_data, valid_data, test_data), \r\n",
        "    batch_size = BATCH_SIZE,\r\n",
        "    sort_within_batch = True)\r\n",
        "\r\n",
        "# Treina o modelo com LSTM\r\n",
        "process(model2, train_with_sequences, evaluate_with_sequences, 'tf-model2')\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 01 | tempo de processamento: 64m 53s\n",
            "\tTrain Loss: 0.666 | Train Acc: 59.30%\n",
            "\t Val. Loss: 0.623 |  Val. Acc: 65.73%\n",
            "Época: 02 | tempo de processamento: 58m 24s\n",
            "\tTrain Loss: 0.605 | Train Acc: 67.23%\n",
            "\t Val. Loss: 0.609 |  Val. Acc: 67.92%\n",
            "Época: 03 | tempo de processamento: 66m 14s\n",
            "\tTrain Loss: 0.590 | Train Acc: 68.71%\n",
            "\t Val. Loss: 0.697 |  Val. Acc: 66.06%\n",
            "Época: 04 | tempo de processamento: 57m 56s\n",
            "\tTrain Loss: 0.471 | Train Acc: 78.66%\n",
            "\t Val. Loss: 0.415 |  Val. Acc: 82.54%\n",
            "Época: 05 | tempo de processamento: 63m 9s\n",
            "\tTrain Loss: 0.406 | Train Acc: 82.31%\n",
            "\t Val. Loss: 0.414 |  Val. Acc: 83.66%\n"
          ]
        }
      ],
      "metadata": {
        "id": "a8McPbOQ8HFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análise segundo algoritmo\r\n",
        "\r\n",
        "Aqui já conseguimos uma acurácia boa usando nosso vocabulário usando gensim. \r\n",
        "Resultado muito similar ao exemplo que utiliza o GloVe. Dessa forma não vi necessidade de otimizações. \r\n",
        "Até porque os tempos de processamento (aprox. 1h por época) torna esse processo inviável.\r\n",
        "\r\n",
        "Outras possibilidades:\r\n",
        "- Otimizar o dicionário para remover palavras irrelevantes para análise. Por exemplo, \r\n",
        "\"film\", ou \"movie\" que não direcionam para uma classificação específica. E priorizar outras que, com certeza direcionam,\r\n",
        "como \"good\" ou \"horrible\". Pois a necessidade da nossa classificação é binária. Bom ou ruim. Não é multi-classe.\r\n",
        "- Melhorar a velocidade do algoritmo usando técnicas como FastText (dica de [bentrevet](https://arxiv.org/abs/1607.01759));\r\n",
        "- Usar um algoritmo convulational (CNNs) - que [mostra resultados excelentes com alguns ajustes em hiperparâmetros](https://arxiv.org/abs/1408.5882).\r\n",
        "- Outra melhoria seria reduzir o dicionário e usar o algoritmo de proximidade para palavras que não entraram no dicionário. Por exemplo, no dicionário tem \"bad\" mas não \"terrible\". No algoritmo a feature seria substituída por \"bad\" por ser a mais similar. Acredito que haja algo assim no Embeeded, mas temos um máximo de palavras no vocabulário. Então com certeza algo se perde.\r\n",
        "\r\n",
        "Mas, de novo, o tempo de treino dificulta demais esses testes. \r\n",
        "\r\n",
        "Dessa forma seguimos com a rotina de predição solicitada para o trabalho."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "source": [
        "# Testando o modelo\r\n",
        "# Exemplo de persistência: https://pytorch.org/tutorials/beginner/saving_loading_models.html\r\n",
        "\r\n",
        "model_test = MultiLayerRNN(INPUT_DIM, \r\n",
        "                EMBEDDING_DIM, \r\n",
        "                HIDDEN_DIM, \r\n",
        "                OUTPUT_DIM, \r\n",
        "                N_LAYERS, \r\n",
        "                BIDIRECTIONAL, \r\n",
        "                DROPOUT, \r\n",
        "                PAD_IDX)\r\n",
        "model_test.load_state_dict(torch.load('tf-model2.pt'))\r\n",
        "\r\n",
        "test_loss, test_acc = evaluate_with_sequences(model_test, test_iterator, criterion)\r\n",
        "\r\n",
        "print(f'Teste Loss: {test_loss:.3f} | Teste Acc: {test_acc*100:.2f}%')\r\n",
        "\r\n",
        "print('Informações relevantes que não são salvas no estado')\r\n",
        "print('INPUT_DIM=', INPUT_DIM, ', PAD_IDX=', PAD_IDX)\r\n",
        "\r\n",
        "# Precisamos salvar o vocabulário para reuso na predição\r\n",
        "torch.save(TEXT.vocab, \"vocab2.pt\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teste Loss: 0.393 | Teste Acc: 83.17%\n",
            "Informações relevantes que não são salvas no estado\n",
            "INPUT_DIM= 30002 , PAD_IDX= 1\n"
          ]
        }
      ],
      "metadata": {
        "id": "Hc3cKyZrhRcg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "83,17% de acurácia treinando com o próprio IMDB usando GenSim pra stemming."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "source": [
        "# Obs.: Para executar esta célula somente é necessário declarar a classe e importar as libs acima.\r\n",
        "\r\n",
        "# Função para ficar claro onde fica a parte de carregamento de estado\r\n",
        "def prepare_model():\r\n",
        "    def tokenize(sentence):\r\n",
        "        return gensim.parsing.preprocessing.preprocess_string(sentence)\r\n",
        "\r\n",
        "    TEXT = data.Field(tokenize = tokenize,\r\n",
        "                      include_lengths = True)\r\n",
        "\r\n",
        "    TEXT.vocab = torch.load(\"vocab2.pt\")\r\n",
        "\r\n",
        "    model = torch.load('tf-model2.ptf')\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    return model, TEXT\r\n",
        "\r\n",
        "# Rotina independente de predição\r\n",
        "def predict(text : str):\r\n",
        "    # Isso poderia ser separado para performance\r\n",
        "    model_pred, TEXT = prepare_model()\r\n",
        "\r\n",
        "    texts, lengths = TEXT.process([text])\r\n",
        "    with torch.no_grad():\r\n",
        "        preds = model_pred(texts, lengths).squeeze(1)\r\n",
        "\r\n",
        "    return torch.sigmoid(preds).round()\r\n",
        "\r\n",
        "tests = ['very good film', \r\n",
        "    'not bad movie', \r\n",
        "    'terrible acting', \r\n",
        "    \"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked\",\r\n",
        "    \"A wonderful little production\"]\r\n",
        "for t in tests:\r\n",
        "    s = predict(t)\r\n",
        "    print('Teste:', t, '[POS]' if s else '[NEG]')\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teste: very good film [POS]\n",
            "Teste: not bad movie [POS]\n",
            "Teste: terrible acting [NEG]\n",
            "Teste: One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked [NEG]\n",
            "Teste: A wonderful little production [NEG]\n"
          ]
        }
      ],
      "metadata": {}
    }
  ]
}